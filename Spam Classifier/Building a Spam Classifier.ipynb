{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to build a spam classifier for e-mail filtering.\n",
    "\n",
    "For this task, we use a training data set composed of 2646 e-mails, where 1214 were previously labeled as \"spam\" and 1432 as \"notspam\".\n",
    "\n",
    "A \"bag of words\" model is used to codify each e-mail message as a set of textual features, and then the Naive Bayes learning algorithm is used to learn the predictive model. Two variations of the Naive Bayes algorithm are tried: the so called Multinomial Naive Bayes and Bernoulli Naive Bayes.\n",
    "\n",
    "The predictive models are evaluated in a test set containing 2554 e-mails, where 1185 are \"spam\" and 1369 are \"notspam\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we work with textual data, lots of preprocessing may be required depending on the application. In this case, as we're going to use the \"bag of words\" model, we only need to get rid of some unnecessary symbols and words, do some stemming to reduce the feature domain, and then codify the results to TF-IDF (term frequency-inverse document frequency) vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'\n",
    "target_names = ['notspam', 'spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " 'should',\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " 'couldn',\n",
       " 'didn',\n",
       " 'doesn',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'haven',\n",
       " 'isn',\n",
       " 'ma',\n",
       " 'mightn',\n",
       " 'mustn',\n",
       " 'needn',\n",
       " 'shan',\n",
       " 'shouldn',\n",
       " 'wasn',\n",
       " 'weren',\n",
       " 'won',\n",
       " 'wouldn']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These words won't add anything to the model, so they may be removed.\n",
    "stopwords_en = stopwords.words('english')\n",
    "stopwords_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all punctuation characters except $, % and &, because perhaps these\n",
    "# ones (especially $) may provide good hints that a message is a spam.\n",
    "punct = ''.join([x for x in string.punctuation if x not in '$%&'])\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Other punctuation characters are going to be erased.\n",
    "punct_trans = str.maketrans(dict.fromkeys(punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There are a bunch of stemming algorithms out there.\n",
    "# I'll stick to Lancaster's one.\n",
    "stemmer = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This dictionary will organize all cleaned data to make things easier later.\n",
    "df_dict = {\n",
    "    'train': {'target': [], 'text': []},\n",
    "    'test': {'target': [], 'text': []},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    \"\"\"Apply some basic steps for text cleaning, by tokenizing words,\n",
    "    removing punctuation, removing stopwords, and doing word stemming.\n",
    "    \"\"\"\n",
    "    # Punctuation removal.\n",
    "    text = text.translate(punct_trans)\n",
    "\n",
    "    # Word tokenization + stopwords removal. \n",
    "    words = word_tokenize(text, language='english')\n",
    "    words = [x for x in words if x not in stopwords_en]\n",
    "\n",
    "    # Word stemming.\n",
    "    stems = [stemmer.stem(x) for x in words]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The training and test data are spread over multiple files and directories.\n",
    "# File contents are encoded with latin-1 encoding.\n",
    "for dataset in os.listdir(data_dir):\n",
    "    dataset_dir = os.path.join(data_dir, dataset)\n",
    "\n",
    "    for target_name in os.listdir(dataset_dir):\n",
    "        target_dir = os.path.join(dataset_dir, target_name)\n",
    "\n",
    "        for filename in os.listdir(target_dir):\n",
    "            with open(os.path.join(target_dir, filename), encoding='latin-1') as file:                \n",
    "                text = clean(file.read())\n",
    "                target = target_names.index(target_name)\n",
    "\n",
    "                df_dict[dataset]['text'].append(text)\n",
    "                df_dict[dataset]['target'].append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>from yourhealthhottmailcom fri sep 20 114132 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>from ihjkhangel92470yahoocom mon jun 24 174856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>from nonenonecom mon sep 2 162801 2002 returnp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>from forkadminxentcom wed jul 3 120744 2002 re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>from tim3h85kg9bigfootcom wed dec 4 115840 200...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text\n",
       "0       1  from yourhealthhottmailcom fri sep 20 114132 2...\n",
       "1       1  from ihjkhangel92470yahoocom mon jun 24 174856...\n",
       "2       1  from nonenonecom mon sep 2 162801 2002 returnp...\n",
       "3       1  from forkadminxentcom wed jul 3 120744 2002 re...\n",
       "4       1  from tim3h85kg9bigfootcom wed dec 4 115840 200..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now it's easy to create DataFrames that put data together in a good shape.\n",
    "df_train = pd.DataFrame(df_dict['train'])\n",
    "df_test = pd.DataFrame(df_dict['test'])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2646, 2554)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = df_train.shape[0]\n",
    "test_size = df_test.shape[0]\n",
    "\n",
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Finally, the TF-IDF feature encoding.\n",
    "# We codify only the 1000 most frequent terms (at most) as features.\n",
    "tfidf_vect = TfidfVectorizer(encoding='latin-1', max_features=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2646, 1000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learn the features and transform the training data.\n",
    "X_train = tfidf_vect.fit_transform(df_train['text'])\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2646,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the known outputs.\n",
    "y_train = df_train['target'].data\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preprocessing the data, the Naive Bayes algorithm can finally be used to learn the predictive models.\n",
    "\n",
    "What distinguishes Multinomial Naive Bayes from Bernoulli Naive Bayes is basically the way each algorithm takes the feature words into account in their computations: the former considers the relative frequency of each feature, while the latter just checks whether a feature is contained in the input text or not.\n",
    "\n",
    "Thanks to Scikit-learn (and the fact that we don't need to worry with data sampling, cross validation and parameter tuning in this project), training the classifiers (predictive models) is extremely easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model = MultinomialNB()\n",
    "mnb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_model = BernoulliNB()\n",
    "bnb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now both models can be evaluated in the test set, which contains only e-mail messages that are completely unknown to the models.\n",
    "\n",
    "There are plenty of ways to evaluate the results of a binary classifier. Here we use the confusion matrix, the precision-recall scores, the F1-score and the area under the ROC curve. The results are tested both in the training set and in the test set (even though only the latter really matters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2554, 1000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data to the SAME space of features learned in the training step.\n",
    "# This is crucial!\n",
    "X_test = tfidf_vect.transform(df_test['text'])\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2554,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the expected results.\n",
    "y_test = df_test['target'].data\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just some labels for the confusion matrices.\n",
    "index = [np.array(['actual', 'actual']), np.array(['0', '1'])]\n",
    "columns = [np.array(['predicted', 'predicted']), np.array(['0', '1'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnb_pred_train = mnb_model.predict(X_train)\n",
    "mnb_pred_test = mnb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         predicted      \n",
      "                 0     1\n",
      "actual 0      1343    89\n",
      "       1        45  1169\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.94      0.95      1432\n",
      "          1       0.93      0.96      0.95      1214\n",
      "\n",
      "avg / total       0.95      0.95      0.95      2646\n",
      "\n",
      "AUC = 0.950\n"
     ]
    }
   ],
   "source": [
    "# Evaluation in the training data set.\n",
    "print(pd.DataFrame(confusion_matrix(y_train, mnb_pred_train), index=index, columns=columns))\n",
    "print()\n",
    "print(classification_report(y_train, mnb_pred_train))\n",
    "print('AUC = %.3f' % roc_auc_score(y_train, mnb_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         predicted      \n",
      "                 0     1\n",
      "actual 0      1263   106\n",
      "       1        65  1120\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.92      0.94      1369\n",
      "          1       0.91      0.95      0.93      1185\n",
      "\n",
      "avg / total       0.93      0.93      0.93      2554\n",
      "\n",
      "AUC = 0.934\n"
     ]
    }
   ],
   "source": [
    "# Evaluation in the test data set.\n",
    "print(pd.DataFrame(confusion_matrix(y_test, mnb_pred_test), index=index, columns=columns))\n",
    "print()\n",
    "print(classification_report(y_test, mnb_pred_test))\n",
    "print('AUC = %.3f' % roc_auc_score(y_test, mnb_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bnb_pred_train = bnb_model.predict(X_train)\n",
    "bnb_pred_test = bnb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         predicted      \n",
      "                 0     1\n",
      "actual 0      1310   122\n",
      "       1        57  1157\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.91      0.94      1432\n",
      "          1       0.90      0.95      0.93      1214\n",
      "\n",
      "avg / total       0.93      0.93      0.93      2646\n",
      "\n",
      "AUC = 0.934\n"
     ]
    }
   ],
   "source": [
    "# Evaluation in the training data set.\n",
    "print(pd.DataFrame(confusion_matrix(y_train, bnb_pred_train), index=index, columns=columns))\n",
    "print()\n",
    "print(classification_report(y_train, bnb_pred_train))\n",
    "print('AUC = %.3f' % roc_auc_score(y_train, bnb_pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         predicted      \n",
      "                 0     1\n",
      "actual 0      1234   135\n",
      "       1        69  1116\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.90      0.92      1369\n",
      "          1       0.89      0.94      0.92      1185\n",
      "\n",
      "avg / total       0.92      0.92      0.92      2554\n",
      "\n",
      "AUC = 0.922\n"
     ]
    }
   ],
   "source": [
    "# Evaluation in the test data set.\n",
    "print(pd.DataFrame(confusion_matrix(y_test, bnb_pred_test), index=index, columns=columns))\n",
    "print()\n",
    "print(classification_report(y_test, bnb_pred_test))\n",
    "print('AUC = %.3f' % roc_auc_score(y_test, bnb_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier that performs well only in the training data set would be useless. Fortunately, this was not the case at all. Even though we used a pretty simple approach which represents texts as a mere set of indepedent words (something that is definitely not true in the real world!), both models achieved great results. Precision, recall, F1-score and the AUC value got all above 0.90 even in test set. This shows how good the Naive Bayes algorithm (and its variations) can be for building text-based predictive models.\n",
    "\n",
    "Moreover, we can see that the Multinomal Naive Bayes model is slightly better than the other one, and that most mistakes that both models made were false negatives (i.e., messages that were not spam but were classified as spams). Had we set specific values for the class priors (assuming, for example, that spams are probabilistic rarer than non-spam messages), then PERHAPS these results could be improved even more (they would if the assumption was correct).\n",
    "\n",
    "Just to finish, I put below some additional examples of usage..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    \"Hey, dude, what's up? It would be nice to hear from you!\",\n",
    "    \"Buy our new exclusive product for only $200.\",\n",
    "    \"Congratulations, you just won a free ticket to Hell! Call 666 to claim it.\",\n",
    "    \"Fishes really stink, scientists say.\",\n",
    "]\n",
    "\n",
    "features = tfidf_vect.transform([clean(x) for x in messages])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model.predict(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnb_model.predict(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multinomal Naive Bayes correctly classified all of them, but the Bernoulli Naive Bayes mistakenly took the 1st and 4th messages as spams. It looks like just checking the presence/abscence of words is not as a good approach as computing the word frequencies (at least for this particular example). By the way, the Bernoulli Naive Bayes model did perform very well in the real e-mails anyway."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
