{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Sentiment Analysis on Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this mini project is to collect data from Twitter and perform sentiment analysis on tweets in real-time using the Spark Streaming API. A predictive model classifies the downloaded tweets as being indicators of either positive (1) or negative (0) feelings.\n",
    "\n",
    "For this task, a corpus (data set) with over 1.5 million prelabeled tweets was collected from a [Kaggle competition](https://inclass.kaggle.com/c/si650winter11) hosted by the University of Michigan. The Naive Bayes algorithm is then used to train the model on such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PySpark classes.\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK classes and functions.\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modules for HTTP requests.\n",
    "import requests_oauthlib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Miscellaneous modules.\n",
    "import operator\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training CSV file to an RDD.\n",
    "rdd = sc.textFile('labeled_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ItemID,Sentiment,SentimentSource,SentimentText',\n",
       " '1,0,Sentiment140,                     is so sad for my APL friend.............',\n",
       " '2,0,Sentiment140,                   I missed the New Moon trailer...',\n",
       " '3,1,Sentiment140,              omg its already 7:30 :O',\n",
       " \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\",\n",
       " '5,0,Sentiment140,         i think mi bf is cheating on me!!!       T_T',\n",
       " '6,0,Sentiment140,         or i just worry too much?        ',\n",
       " '7,1,Sentiment140,       Juuuuuuuuuuuuuuuuussssst Chillin!!',\n",
       " '8,0,Sentiment140,       Sunny Again        Work Tomorrow  :-|       TV Tonight',\n",
       " '9,1,Sentiment140,      handed in my uniform today . i miss you already']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first rows.\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the header.\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,0,Sentiment140,                     is so sad for my APL friend.............',\n",
       " '2,0,Sentiment140,                   I missed the New Moon trailer...',\n",
       " '3,1,Sentiment140,              omg its already 7:30 :O',\n",
       " \"4,0,Sentiment140,          .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\",\n",
       " '5,0,Sentiment140,         i think mi bf is cheating on me!!!       T_T',\n",
       " '6,0,Sentiment140,         or i just worry too much?        ',\n",
       " '7,1,Sentiment140,       Juuuuuuuuuuuuuuuuussssst Chillin!!',\n",
       " '8,0,Sentiment140,       Sunny Again        Work Tomorrow  :-|       TV Tonight',\n",
       " '9,1,Sentiment140,      handed in my uniform today . i miss you already',\n",
       " '10,1,Sentiment140,      hmmmm.... i wonder how she my number @-)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of English stopwords with and without negation marks.\n",
    "all_stopwords = sorted(\n",
    "    stopwords.words('english') + [word + '_NEG' for word in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_row(row):\n",
    "    \"\"\"Extract the tweet contents and the sentiment label from a row.\n",
    "    \"\"\"\n",
    "    row = row.split(',')\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    # Remove whitespaces, stopwords, punctuation, and convert to lowercase.\n",
    "    tweet = re.sub(' +', ' ', row[3]).lower()\n",
    "    tweet = mark_negation(tweet).translate(translator).split(' ')\n",
    "    tweet = [word for word in tweet if word != '' and word not in all_stopwords]\n",
    "\n",
    "    sentiment = row[1]\n",
    "\n",
    "    return tweet, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the clean function for all rows.\n",
    "rdd = rdd.map(preprocess_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['sad', 'apl', 'friend'], '0'),\n",
       " (['missed', 'new', 'moon', 'trailer'], '0'),\n",
       " (['omg', 'already', '730'], '1'),\n",
       " (['omgaga',\n",
       "   'im',\n",
       "   'sooo',\n",
       "   'im',\n",
       "   'gunna',\n",
       "   'cry',\n",
       "   'ive',\n",
       "   'dentist',\n",
       "   'since',\n",
       "   '11',\n",
       "   'suposed',\n",
       "   '2',\n",
       "   'get',\n",
       "   'crown',\n",
       "   'put',\n",
       "   '30mins'],\n",
       "  '0'),\n",
       " (['think', 'mi', 'bf', 'cheating', 'tt'], '0'),\n",
       " (['worry', 'much'], '0'),\n",
       " (['juuuuuuuuuuuuuuuuussssst', 'chillin'], '1'),\n",
       " (['sunny', 'work', 'tomorrow', 'tv', 'tonight'], '0'),\n",
       " (['handed', 'uniform', 'today', 'miss', 'already'], '1'),\n",
       " (['hmmmm', 'wonder', 'number'], '1')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the results.\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the training and test sets.\n",
    "train_rdd, test_rdd = rdd.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the analyzer.\n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all words in the training data.\n",
    "train_data = train_rdd.take(200000)\n",
    "train_words = sentiment_analyzer.all_words(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the top 1000 word features.\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(train_words, top_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the feature extractor.\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract word features in the training data.\n",
    "train_feats = sentiment_analyzer.apply_features(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n",
      "Saving True\n"
     ]
    }
   ],
   "source": [
    "# Train the Naive Bayes model.\n",
    "nb_trainer = NaiveBayesClassifier.train\n",
    "nb_model = sentiment_analyzer.train(nb_trainer, train_feats, save_classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the test data.\n",
    "test_data = test_rdd.take(50000)\n",
    "test_feats = sentiment_analyzer.apply_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.71364\n",
      "F-measure [0]: 0.6273295158771474\n",
      "F-measure [1]: 0.7674894446248782\n",
      "Precision [0]: 0.7012102874432677\n",
      "Precision [1]: 0.7201499360029255\n",
      "Recall [0]: 0.5675332014693416\n",
      "Recall [1]: 0.8214906486824723\n"
     ]
    }
   ],
   "source": [
    "# Make predictions and evaluate in the test set.\n",
    "test_results = sentiment_analyzer.evaluate(test_feats, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stream update interval (in seconds).\n",
    "MINIBATCH_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maximum number of tweets downloaded at once.\n",
    "TWEETS_PER_BATCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're interested in tweets containing this term.\n",
    "search_term = 'Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the streaming context.\n",
    "ssc = StreamingContext(sc, MINIBATCH_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure the stream.\n",
    "empty_rdd = sc.parallelize([0])\n",
    "stream = ssc.queueStream([], default=empty_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consumer keys and access tokens for the Twitter API.\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Twitter URLs.\n",
    "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
    "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track=' + search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the authentication object.\n",
    "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret,\n",
    "                                access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "    \"\"\"Connect to Twitter and download a certain number of tweets.\n",
    "    \"\"\"\n",
    "    response = requests.get(filter_url, auth=auth, stream=True)\n",
    "    print(filter_url, response)\n",
    "    \n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        if count >= TWEETS_PER_BATCH:\n",
    "            break\n",
    "        try:\n",
    "            post = json.loads(line.decode('utf-8'))\n",
    "            count += 1\n",
    "            yield post['text']\n",
    "        except:\n",
    "            result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify the data transform functions.\n",
    "stream = stream.transform(lambda _, rdd: rdd.flatMap(lambda __: get_tweets()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_classify(row):\n",
    "    \"\"\"Get tweets, put them in the right format and classify them.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    # Remove whitespaces, stopwords, punctuation, and convert to lowercase.\n",
    "    tweet = re.sub(' +', ' ', row).lower()\n",
    "    tweet = mark_negation(tweet).translate(translator).split(' ')\n",
    "    tweet = [word for word in tweet if word != '' and word not in all_stopwords]\n",
    "\n",
    "    # Get the tweet features.\n",
    "    data = [(tweet, '')]\n",
    "    data = sentiment_analyzer.apply_features(data)\n",
    "\n",
    "    # Classify it using the Naive Bayes model.\n",
    "    sentiment = nb_model.classify(data[0][0])\n",
    "    print(tweet, sentiment)\n",
    "\n",
    "    return tweet, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All results will be stored in this list.\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_results(rdd):\n",
    "    \"\"\"Count predictions of each class and add them to the results list.\n",
    "    \"\"\"\n",
    "    global results\n",
    "\n",
    "    # Count tweets classified as 0 and 1.\n",
    "    sentiments_rdd = rdd.map(lambda row: (preprocess_and_classify(row)[1], 1))\n",
    "    counts_rdd = sentiments_rdd.reduceByKey(operator.add)\n",
    "\n",
    "    # Add these counts to the global results.\n",
    "    result = [time.strftime(\"%I:%M:%S\"), counts_rdd.collect()]\n",
    "    results.append(result)\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify the function that runs for each minibatch.\n",
    "stream.foreachRDD(lambda _, rdd: update_results(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start the streaming.\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['04:57:11', []]\n",
      "['04:57:21', [('0', 30), ('1', 66)]]\n",
      "['04:57:31', [('1', 79), ('0', 15)]]\n",
      "['04:57:41', [('1', 74), ('0', 22)]]\n",
      "['04:57:50', [('1', 79), ('0', 17)]]\n",
      "['04:58:01', [('1', 74), ('0', 22)]]\n",
      "['04:58:10', [('1', 75), ('0', 21)]]\n",
      "['04:58:20', []]\n",
      "['04:58:30', [('1', 78), ('0', 18)]]\n",
      "['04:58:41', [('0', 28), ('1', 68)]]\n",
      "['04:58:51', [('1', 76), ('0', 21)]]\n"
     ]
    }
   ],
   "source": [
    "# Wait just until we get a few minibatches.\n",
    "while True:\n",
    "    if len(results) > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['04:57:11', []],\n",
       " ['04:57:21', [('0', 30), ('1', 66)]],\n",
       " ['04:57:31', [('1', 79), ('0', 15)]],\n",
       " ['04:57:41', [('1', 74), ('0', 22)]],\n",
       " ['04:57:50', [('1', 79), ('0', 17)]],\n",
       " ['04:58:01', [('1', 74), ('0', 22)]],\n",
       " ['04:58:10', [('1', 75), ('0', 21)]],\n",
       " ['04:58:20', []],\n",
       " ['04:58:30', [('1', 78), ('0', 18)]],\n",
       " ['04:58:41', [('0', 28), ('1', 68)]],\n",
       " ['04:58:51', [('1', 76), ('0', 21)]]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the results in an RDD.\n",
    "results_rdd = sc.parallelize(results)\n",
    "results_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results in a text file.\n",
    "filename = 'r' + time.strftime(\"%I%M%S\")\n",
    "results_rdd.saveAsTextFile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stop the streaming.\n",
    "ssc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
