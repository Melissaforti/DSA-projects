{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Time Sentiment Analysis on Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this mini project is to collect data from Twitter and perform sentiment analysis on tweets in real-time using the Spark Streaming API. A predictive model classifies the downloaded tweets as being indicators of either positive (1) or negative (0) feelings.\n",
    "\n",
    "For this task, a corpus (data set) with over 1.5 million prelabeled tweets was collected from a [Kaggle competition](https://inclass.kaggle.com/c/si650winter11) hosted by the University of Michigan. The Naive Bayes algorithm is then used to train the model on such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PySpark classes.\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLTK classes and functions.\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Modules for HTTP requests.\n",
    "import requests_oauthlib\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Miscellaneous modules.\n",
    "import operator\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "import ast\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training CSV file to an RDD.\n",
    "rdd = sc.textFile('labeled_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the first rows.\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the header.\n",
    "header = rdd.first()\n",
    "rdd = rdd.filter(lambda row: row != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of English stopwords with and without negation marks.\n",
    "all_stopwords = sorted(\n",
    "    stopwords.words('english') + [word + '_NEG' for word in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_row(row):\n",
    "    \"\"\"Extract the tweet contents and the sentiment label from a row.\n",
    "    \"\"\"\n",
    "    row = row.split(',')\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    # Remove whitespaces, stopwords, punctuation, and convert to lowercase.\n",
    "    tweet = re.sub(' +', ' ', row[3]).lower()\n",
    "    tweet = mark_negation(tweet).translate(translator).split(' ')\n",
    "    tweet = [word for word in tweet if word != '' and word not in all_stopwords]\n",
    "\n",
    "    sentiment = row[1]\n",
    "\n",
    "    return tweet, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply the clean function for all rows.\n",
    "rdd = rdd.map(preprocess_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show the results.\n",
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the training and test sets.\n",
    "train_rdd, test_rdd = rdd.randomSplit([0.7, 0.3], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the analyzer.\n",
    "sentiment_analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get all words in the training data.\n",
    "train_data = train_rdd.collect()\n",
    "train_words = sentiment_analyzer.all_words(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the top 1000 word features.\n",
    "unigram_feats = sentiment_analyzer.unigram_word_feats(train_words, top_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the feature extractor.\n",
    "sentiment_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract word features in the training data.\n",
    "train_feats = sentiment_analyzer.apply_features(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train the Naive Bayes model.\n",
    "nb_trainer = NaiveBayesClassifier.train\n",
    "nb_model = sentiment_analyzer.train(nb_trainer, train_feats, save_classifier=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the test data.\n",
    "test_data = test_rdd.collect()\n",
    "test_feats = sentiment_analyzer.apply_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make predictions and evaluate in the test set.\n",
    "test_results = sentiment_analyzer.evaluate(test_feats, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stream update interval (in seconds).\n",
    "MINIBATCH_INTERVAL = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Maximum number of tweets downloaded at once.\n",
    "TWEETS_PER_BATCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We're interested in tweets containing this term.\n",
    "search_term = 'Trump'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the streaming context.\n",
    "ssc = StreamingContext(sc, MINIBATCH_INTERVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Configure the stream.\n",
    "empty_rdd = sc.parallelize([0])\n",
    "stream = ssc.queueStream([], default=empty_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Consumer keys and access tokens for the Twitter API.\n",
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Twitter URLs.\n",
    "sample_url = 'https://stream.twitter.com/1.1/statuses/sample.json'\n",
    "filter_url = 'https://stream.twitter.com/1.1/statuses/filter.json?track=' + search_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the authentication object.\n",
    "auth = requests_oauthlib.OAuth1(consumer_key, consumer_secret,\n",
    "                                access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets():\n",
    "    \"\"\"Connect to Twitter and download a certain number of tweets.\n",
    "    \"\"\"\n",
    "    response = requests.get(filter_url, auth=auth, stream=True)\n",
    "    print(filter_url, response)\n",
    "    \n",
    "    count = 0\n",
    "    for line in response.iter_lines():\n",
    "        if count >= TWEETS_PER_BATCH:\n",
    "            break\n",
    "        try:\n",
    "            post = json.loads(line.decode('utf-8'))\n",
    "            count += 1\n",
    "            yield post['text']\n",
    "        except:\n",
    "            result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify the data transform functions.\n",
    "stream = stream.transform(lambda _, rdd: rdd.flatMap(lambda __: get_tweets()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_and_classify(row):\n",
    "    \"\"\"Get tweets, put them in the right format and classify them.\n",
    "    \"\"\"\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "    # Remove whitespaces, stopwords, punctuation, and convert to lowercase.\n",
    "    tweet = re.sub(' +', ' ', row).lower()\n",
    "    tweet = mark_negation(tweet).translate(translator).split(' ')\n",
    "    tweet = [word for word in tweet if word != '' and word not in all_stopwords]\n",
    "\n",
    "    # Get the tweet features.\n",
    "    data = [(tweet, '')]\n",
    "    data = sentiment_analyzer.apply_features(data)\n",
    "\n",
    "    # Classify it using the Naive Bayes model.\n",
    "    sentiment = nb_model.classify(data[0][0])\n",
    "    print(tweet, sentiment)\n",
    "\n",
    "    return tweet, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# All results will be stored in this list.\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_results(rdd):\n",
    "    \"\"\"Count predictions of each class and add them to the results list.\n",
    "    \"\"\"\n",
    "    global results\n",
    "\n",
    "    # Count tweets classified as 0 and 1.\n",
    "    sentiments_rdd = rdd.map(lambda row: (preprocess_and_classify(row)[1], 1))\n",
    "    counts_rdd = sentiments_rdd.reduceByKey(operator.add)\n",
    "\n",
    "    # Add these counts to the global results.\n",
    "    result = [time.strftime(\"%I:%M:%S\"), counts_rdd.collect()]\n",
    "    results.append(result)\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify the function that runs for each minibatch.\n",
    "stream.foreachRDD(lambda _, rdd: update_results(rdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Start the streaming.\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Wait just until we get a few minibatches.\n",
    "while True:\n",
    "    if len(results) > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the results in an RDD.\n",
    "results_rdd = sc.parallelize(results)\n",
    "results_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the results in a text file.\n",
    "filename = 'r' + time.strftime(\"%I%M%S\")\n",
    "results_rdd.saveAsTextFile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stop the streaming.\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
